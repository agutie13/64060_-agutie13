---
title: "Assignment_5"
author: "Andrew Gutierrez"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Installing requisite libraries:

``` {r install}
library(cluster)

```

First, I'll read the Cereals.csv file into a DataFrame in R:

```{r dataframe}
cereals = read.csv("C:\\Users\\gutiera9\\Documents\\MSBA KSU\\Cereals.csv",header=T,sep=",")

# set row names to the "utilities"name" column
row.names(cereals) <- cereals[,1]

head(cereals)
```
I'll then pre-process the data by removing all blank values from the dataframe.

```{r remove}
cereals <- na.omit(cereals)
head(cereals)

```
● Apply hierarchical clustering to the data using Euclidean distance to the normalized
measurements. Use Agnes to compare the clustering from single linkage, complete
linkage, average linkage, and Ward. Choose the best method.

My first step here will be to normalize the numerical variable values, and then to compute the Euclidean distance

``` {r Euclidean}
cereals[,c(4:16)] <- scale(cereals[,c(4:16)]) # normalize

dist <- dist(cereals, method = "euclidean") # compute euclidean

```

Now, using the hclust function, I'll compare the single linkage, complete linkage, average linkage, and Ward method plots.

``` {r  hclust}
single <- hclust(dist, method = "single") # single linkage
plot(single, cex = 0.6, hang = -1, main="Single Linkage Dendogram")

complete <- hclust(dist, method = "complete") # complete linkage
plot(complete, cex = 0.6, hang = -1, main="Complete Linkage Dendogram")

average <- hclust(dist, method = "average") # average linkage
plot(average, cex = 0.6, hang = -1, main="Average Linkage Dendogram")

ward <- hclust(dist, method = "ward.D") # ward method
plot(ward, cex = 0.6, hang = -1, main="Ward Linkage Dendogram")

```

Based on the four plots, I believe the Ward method is the best method, as it clearly features the strongest clustering structure from among the four methods. This is borne out by a look at the agglomerative coefficients for each of the methods when applied to the dataset.

``` {r coefficient}
single_coef <- agnes(dist, method = "single")
complete_coef <- agnes(dist, method = "complete")
average_coef <- agnes(dist, method = "average")
ward_coef <- agnes(dist, method = "ward")

print("Single Linkage Coefficient: ")
print(single_coef$ac)

print("Complete Linkage Coefficient: ")
print(complete_coef$ac)

print("Average Linkage Coefficient: ")
print(average_coef$ac)

print("Ward Method Coefficient: ")
print(ward_coef$ac)

```

Of the four linkages the Ward method has the highest agglomerative coefficient of 0.9, backing up my choice that it is the best form of method for this particular dataset.

● How many clusters would you choose?

To my eye, I see four clear clusters on the ward dendogram. Drawing the dendogram with borders around the clusters makes this apparent (see below).

``` {r cluster}
plot(ward, cex = 0.6, hang = -1, main="Ward Linkage Dendogram")
rect.hclust(ward, k=4, border = 1:4)

```

Comment on the structure of the clusters and on their stability. Hint: To check stability,  
partition the data and see how well clusters formed based on one part apply to the other 
part. To do this: 
  ● Cluster partition A 
  ● Use the cluster centroids from A to assign each record in partition B (each record 
  is assigned to the cluster with the closest centroid). 
  ● Assess how consistent the cluster assignments are compared to the 
  assignments based on all the data. 

First, I'll partition the dataset into two partitions (A and B). Then, I'll cluster the first partition using the Ward method like before.

``` {r partition}
# partition data and compute euclidean distances
dist_1 <- dist(cereals[0:38,], method = "euclidean") 
dist_2 <- dist(cereals[39:76,], method = "euclidean") 

#cluster first partition
ward_1 <- hclust(dist_1, method = "ward.D")
plot(ward_1, cex = 0.6, hang = -1, main="Ward Linkage Dendogram")
rect.hclust(ward_1, k=4, border = 1:4)
```
Let's see how the plot from partition 1 compares with the plot of the overall dataset:

``` {r plots}

### Plot of partition 1
plot(ward_1, cex = 0.6, hang = -1, main="Ward Linkage Dendogram - Partition")
rect.hclust(ward_1, k=4, border = 1:4)

plot(ward, cex = 0.6, hang = -1, main="Ward Linkage Dendogram - Full Dataset")
rect.hclust(ward, k=4, border = 1:4)

```

In comparing the clusters from the partition vs. the clusters from the overall dataset, one finds that the allocation is fairly similar, indicating a general stableness to the hierarchy. For example, the partition has All-Bran with Extra Fiber, 100% Bran, and All-Bran clustered together, as does the full dataset. The partition has Basic 4, Fruitful Bran, and Cracklin' Oat Bran all clustered together, as does the full dataset. And the partition and full dataset both have Honeycombs, Corn-Pops, and Apple Jacks all clustered together. In fact, in reviewing the clusters from the partition, I can't find a single inconsistency where two cereals are clustered together in the partition but NOT in the full dataset, or vice-versa.


● The elementary public schools would like to choose a set of cereals to include in their 
daily cafeterias. Every day a different cereal is offered, but all cereals should support a 
healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” 
Should the data be normalized? If not, how should they be used in the cluster analysis? 

First, I'll calculate the Euclidean distances for a subset of the variables in the dataset that specifically have to do with "healthiness" (specifically calories, protein, fat, sodium, fiber, carbo, sugars, potass, and vitamins). Note that these variables should in fact be normalized, as the scale of variables can affect our hierarchical clustering model.

Once I have those distances, I'll calculate the agglomerative coefficients for the data using the Single, Complete, Average, and Ward Method linkages, in order to determine which method works best with this subset of variables.

``` {r healthy}
disthealth <- dist(cereals[,c(4:12)], method = "euclidean")

single_coef_health <- agnes(disthealth, method = "single")
complete_coef_health <- agnes(disthealth, method = "complete")
average_coef_health <- agnes(disthealth, method = "average")
ward_coef_health <- agnes(disthealth, method = "ward")

print("Single Linkage Coefficient: ")
print(single_coef_health$ac)

print("Complete Linkage Coefficient: ")
print(complete_coef_health$ac)

print("Average Linkage Coefficient: ")
print(average_coef_health$ac)

print("Ward Method Coefficient: ")
print(ward_coef_health$ac)

```

This shows that the Ward method still has the strongest clustering structure, at 0.91.

Now, I'll plot the clusters using the Ward method.

``` {r plot 2}
ward_health <- hclust(disthealth, method = "ward.D") # ward method
plot(ward_health, cex = 0.6, hang = -1, main="Ward Linkage Dendogram")

```
Let's try drawing some borders on this dendogram in order to see two distinct clusters - healthy, and unhealthy.

``` {r borders 2}
plot(ward_health, cex = 0.6, hang = -1, main="Ward Linkage Dendogram")
rect.hclust(ward_health, k=2, border = 1:3)

```
The "healthy" cluster on this dendogram is clearly the second cluster, aka "red". This cluster features cereals that generally have a lower sugar content - including Rice Chex, Corn Flakes, Raisin Nut Bran, Puffed Wheat, and Shredded Wheat. The "unhealthy" cluster, by contrast, features cereals such as Cocoa Puffs, Count Chocula, Fruity Pebbles, and Apple Jacks - all cereals that are notorious for having high sugar content. The elementary public schools would be advised to select cereals only from the second "red" cluster.

